{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c348e-7a45-4dbd-80a4-050360b42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "import openai\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DefaultDataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import RerankingEvaluator\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs.data import TokensPrompt\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccccc97-14e4-4de8-a5cd-b812cb9c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF实现\n",
    "class TFIDF:\n",
    "    def __init__(self, use_jieba=True, custom_dict=None):\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "        self.doc_count = 0\n",
    "        self.doc_term_count = defaultdict(int)\n",
    "        self.use_jieba = use_jieba\n",
    "        self.documents = []\n",
    "        self.doc_vectors = []\n",
    "        if self.use_jieba:\n",
    "            if custom_dict:\n",
    "                jieba.load_userdict(custom_dict)\n",
    "            jieba.add_word('RAG')\n",
    "            jieba.add_word('Retrieval')\n",
    "            jieba.add_word('Augmented')\n",
    "            jieba.add_word('Generation')\n",
    "            jieba.add_word('Passage')\n",
    "            jieba.add_word('seq2seq')\n",
    "            jieba.add_word('BERT')\n",
    "            jieba.add_word('GPT')\n",
    "            jieba.add_word('Transformer')\n",
    "            jieba.add_word('NLP')\n",
    "            \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        if self.use_jieba:\n",
    "            return self._mixed_segmentation(text)\n",
    "        else:\n",
    "            return self._english_segmentation(text)\n",
    "    \n",
    "    def _english_segmentation(self, text):\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.strip()]\n",
    "        return words\n",
    "    \n",
    "    def _mixed_segmentation(self, text):\n",
    "        words = list(jieba.cut(text))\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            if not word:\n",
    "                continue\n",
    "            if self._is_english_word(word):\n",
    "                processed_words.extend(self._process_english_word(word))\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        return processed_words\n",
    "    \n",
    "    def _is_english_word(self, word):\n",
    "        return bool(re.match(r'^[a-z0-9]+$', word))\n",
    "    \n",
    "    def _process_english_word(self, word):\n",
    "        if '-' in word and len(word) > 1:\n",
    "            parts = word.split('-')\n",
    "            return [part for part in parts if len(part) > 1]\n",
    "        if word.endswith('es') and len(word) > 2:\n",
    "            base_word = word[:-2]\n",
    "            if self._is_valid_english_word(base_word):\n",
    "                return [base_word]\n",
    "        elif word.endswith('s') and len(word) > 1:\n",
    "            base_word = word[:-1]\n",
    "            if self._is_valid_english_word(base_word):\n",
    "                return [base_word]\n",
    "        return [word]\n",
    "    \n",
    "    def _is_valid_english_word(self, word):\n",
    "        return len(word) > 1 and word.isalpha()\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        self.documents = documents\n",
    "        self.doc_count = len(documents)\n",
    "        self.vocabulary = set()\n",
    "\n",
    "        doc_words_list = []\n",
    "        for doc in documents:\n",
    "            words = self.preprocess_text(doc)\n",
    "            doc_words_list.append(words)\n",
    "            unique_words = set(words)\n",
    "            self.vocabulary.update(unique_words)\n",
    "            for word in unique_words:\n",
    "                self.doc_term_count[word] += 1\n",
    "\n",
    "        for word in self.vocabulary:\n",
    "            self.idf[word] = math.log(self.doc_count / (self.doc_term_count[word] + 1))\n",
    "\n",
    "        self.doc_vectors = self._compute_document_vectors(documents)\n",
    "        \n",
    "        return doc_words_list\n",
    "    \n",
    "    def _compute_document_vectors(self, documents):\n",
    "        tfidf_matrix = []\n",
    "        for doc in documents:\n",
    "            words = self.preprocess_text(doc)\n",
    "            tf = self.compute_tf(words)\n",
    "            \n",
    "            tfidf_vector = {}\n",
    "            for word in self.vocabulary:\n",
    "                tf_value = tf.get(word, 0)\n",
    "                idf_value = self.idf.get(word, 0)\n",
    "                tfidf_vector[word] = tf_value * idf_value\n",
    "            tfidf_matrix.append(tfidf_vector)\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def compute_tf(self, words):\n",
    "        total_words = len(words)\n",
    "        if total_words == 0:\n",
    "            return {}\n",
    "        word_count = Counter(words)\n",
    "        tf = {}\n",
    "        for word, count in word_count.items():\n",
    "            tf[word] = count / total_words\n",
    "        return tf\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        if not self.idf:\n",
    "            raise ValueError(\"请先调用fit方法训练模型\")\n",
    "        return self._compute_document_vectors(documents)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return sorted(list(self.vocabulary))\n",
    "    \n",
    "    def transform_to_dense_matrix(self, tfidf_matrix=None):\n",
    "        if tfidf_matrix is None:\n",
    "            tfidf_matrix = self.doc_vectors\n",
    "        feature_names = self.get_feature_names()\n",
    "        dense_matrix = []\n",
    "        for vector in tfidf_matrix:\n",
    "            row = [vector.get(word, 0) for word in feature_names]\n",
    "            dense_matrix.append(row)\n",
    "            \n",
    "        return dense_matrix, feature_names\n",
    "    \n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"向量维度不一致\")\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        norm1 = math.sqrt(sum(a * a for a in vec1))\n",
    "        norm2 = math.sqrt(sum(b * b for b in vec2))\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def score(self, query, doc_index):\n",
    "        if doc_index >= len(self.documents):\n",
    "            return 0\n",
    "        query_vector = self._query_to_vector(query)\n",
    "        doc_vector = self._doc_to_vector(doc_index)\n",
    "        return self._cosine_similarity(query_vector, doc_vector)\n",
    "    \n",
    "    def _query_to_vector(self, query):\n",
    "        query_tfidf = self.transform([query])[0]\n",
    "        feature_names = self.get_feature_names()\n",
    "        return [query_tfidf.get(word, 0) for word in feature_names]\n",
    "    \n",
    "    def _doc_to_vector(self, doc_index):\n",
    "        feature_names = self.get_feature_names()\n",
    "        doc_vector = self.doc_vectors[doc_index]\n",
    "        return [doc_vector.get(word, 0) for word in feature_names]\n",
    "    \n",
    "    def search(self, query, top_k=None):\n",
    "        scores = []\n",
    "        for i in range(len(self.documents)):\n",
    "            score = self.score(query, i)\n",
    "            scores.append((i, score, self.documents[i]))\n",
    "        # 按得分降序排序\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return scores[:top_k]\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae274c-c105-41bc-9844-63ff458bc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25实现\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75, use_jieba=True, custom_dict=None):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.documents = []\n",
    "        self.doc_lengths = []\n",
    "        self.avg_doc_length = 0\n",
    "        self.use_jieba = use_jieba\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.term_freqs = []\n",
    "        self.vocabulary = set()\n",
    "        if self.use_jieba:\n",
    "            if custom_dict:\n",
    "                jieba.load_userdict(custom_dict)\n",
    "            jieba.add_word('RAG')\n",
    "            jieba.add_word('Retrieval')\n",
    "            jieba.add_word('Augmented')\n",
    "            jieba.add_word('Generation')\n",
    "            jieba.add_word('Passage')\n",
    "            jieba.add_word('seq2seq')\n",
    "            jieba.add_word('BERT')\n",
    "            jieba.add_word('GPT')\n",
    "            jieba.add_word('Transformer')\n",
    "            jieba.add_word('NLP')\n",
    "            \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        words = list(jieba.cut(text))\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            if not word:\n",
    "                continue\n",
    "            if re.match(r'^[a-z0-9\\u4e00-\\u9fff]+$', word):\n",
    "                processed_words.append(word)\n",
    "        return processed_words\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        self.documents = documents\n",
    "        self.doc_lengths = []\n",
    "        self.term_freqs = []\n",
    "        for doc in documents:\n",
    "            words = self.preprocess_text(doc)\n",
    "            self.doc_lengths.append(len(words))\n",
    "            term_freq = Counter(words)\n",
    "            self.term_freqs.append(term_freq)\n",
    "            for word in set(words):\n",
    "                self.vocabulary.add(word)\n",
    "                self.doc_freqs[word] += 1\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0\n",
    "        self.idf = {}\n",
    "        N = len(documents)\n",
    "        for word in self.vocabulary:\n",
    "            # BM25的IDF公式\n",
    "            self.idf[word] = math.log((N - self.doc_freqs[word] + 0.5) / (self.doc_freqs[word] + 0.5) + 1)\n",
    "    \n",
    "    def score(self, query, doc_index):\n",
    "        if doc_index >= len(self.documents):\n",
    "            return 0\n",
    "        words = self.preprocess_text(query)\n",
    "        score = 0\n",
    "        doc_length = self.doc_lengths[doc_index]\n",
    "        term_freq = self.term_freqs[doc_index]\n",
    "        for word in words:\n",
    "            if word not in self.vocabulary:\n",
    "                continue\n",
    "            f = term_freq.get(word, 0)\n",
    "            numerator = f * (self.k1 + 1)\n",
    "            denominator = f + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))\n",
    "            if denominator > 0:\n",
    "                score += self.idf[word] * (numerator / denominator)\n",
    "        return score\n",
    "    \n",
    "    def search(self, query, top_k=None):\n",
    "        scores = []\n",
    "        for i in range(len(self.documents)):\n",
    "            score = self.score(query, i)\n",
    "            scores.append((i, score, self.documents[i]))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return scores[:top_k]\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc39611-8dcf-41bd-8ecc-f235690f8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "class Word2VecRetrieval:\n",
    "    def __init__(self, use_jieba=True, vector_size=100, window=2, min_count=1, workers=2):\n",
    "        self.use_jieba = use_jieba\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "        self.documents = []\n",
    "        self.doc_vectors = []\n",
    "        self.vocabulary = set()\n",
    "        if self.use_jieba:\n",
    "            jieba.add_word('RAG')\n",
    "            jieba.add_word('Retrieval')\n",
    "            jieba.add_word('Augmented')\n",
    "            jieba.add_word('Generation')\n",
    "            jieba.add_word('Passage')\n",
    "            jieba.add_word('seq2seq')\n",
    "            jieba.add_word('BERT')\n",
    "            jieba.add_word('GPT')\n",
    "            jieba.add_word('Transformer')\n",
    "            jieba.add_word('NLP')\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        if self.use_jieba:\n",
    "            words = list(jieba.cut(text))\n",
    "        else:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "            words = text.split()\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            if not word:\n",
    "                continue\n",
    "            if re.match(r'^[a-z0-9\\u4e00-\\u9fff]+$', word):\n",
    "                processed_words.append(word)\n",
    "        return processed_words\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        self.documents = documents\n",
    "        tokenized_docs = []\n",
    "        for doc in documents:\n",
    "            words = self.preprocess_text(doc)\n",
    "            tokenized_docs.append(words)\n",
    "            self.vocabulary.update(words)\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokenized_docs,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            sg=0\n",
    "        )\n",
    "        self.doc_vectors = self._compute_document_vectors(tokenized_docs)\n",
    "    \n",
    "    def _compute_document_vectors(self, tokenized_docs):\n",
    "        doc_vectors = []\n",
    "        for words in tokenized_docs:\n",
    "            word_vectors = []\n",
    "            for word in words:\n",
    "                if word in self.model.wv:\n",
    "                    word_vectors.append(self.model.wv[word])\n",
    "            if len(word_vectors) > 0:\n",
    "                doc_vector = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                doc_vector = np.zeros(self.vector_size)\n",
    "            doc_vectors.append(doc_vector)\n",
    "        return doc_vectors\n",
    "    \n",
    "    def _query_to_vector(self, query):\n",
    "        words = self.preprocess_text(query)\n",
    "        word_vectors = []\n",
    "        for word in words:\n",
    "            if word in self.model.wv:\n",
    "                word_vectors.append(self.model.wv[word])\n",
    "        if len(word_vectors) > 0:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "    \n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "            return 0.0\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def score(self, query, doc_index):\n",
    "        if doc_index >= len(self.documents):\n",
    "            return 0\n",
    "        query_vector = self._query_to_vector(query)\n",
    "        doc_vector = self.doc_vectors[doc_index]\n",
    "        return self._cosine_similarity(query_vector, doc_vector)\n",
    "    \n",
    "    def search(self, query, top_k=None):\n",
    "        scores = []\n",
    "        for i in range(len(self.documents)):\n",
    "            score = self.score(query, i)\n",
    "            scores.append((i, score, self.documents[i]))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return scores[:top_k]\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a267077-fcd7-4e78-8135-aab081104fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "class BERTRetrieval:\n",
    "    def __init__(self, model_path: str = \"bert-base-uncased\", max_length: int = 512, device: str = None):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.documents = []\n",
    "        self.document_embeddings = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def encode_text(self, text: str) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.cpu()\n",
    "    \n",
    "    def encode_texts(self, texts: List[str], batch_size: int = 32) -> torch.Tensor:\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # 推理\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "                all_embeddings.append(embeddings.cpu())\n",
    "        \n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    \n",
    "    def fit(self, documents: List[str], batch_size: int = 32):\n",
    "        self.documents = documents\n",
    "        self.document_embeddings = self.encode_texts(documents, batch_size)\n",
    "        return self.document_embeddings\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: torch.Tensor, vec2: torch.Tensor) -> float:\n",
    "        if vec1.dim() == 1:\n",
    "            vec1 = vec1.unsqueeze(0)\n",
    "        if vec2.dim() == 1:\n",
    "            vec2 = vec2.unsqueeze(0)\n",
    "        return torch.nn.functional.cosine_similarity(vec1, vec2).item()\n",
    "    \n",
    "    def score(self, query: str, doc_index: int) -> float:\n",
    "        if doc_index >= len(self.documents) or self.document_embeddings is None:\n",
    "            return 0.0\n",
    "        query_embedding = self.encode_text(query)\n",
    "        doc_embedding = self.document_embeddings[doc_index]\n",
    "        similarity = self._cosine_similarity(query_embedding, doc_embedding)\n",
    "        return similarity\n",
    "    \n",
    "    def search(self, query: str, top_k: Optional[int] = None) -> List[Tuple[int, float, str]]:\n",
    "\n",
    "        if self.document_embeddings is None:\n",
    "            raise ValueError(\"请先调用fit方法处理文档\")\n",
    "        query_embedding = self.encode_text(query)\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            query_embedding, \n",
    "            self.document_embeddings\n",
    "        ).tolist()\n",
    "        results = []\n",
    "        for i, similarity in enumerate(similarities):\n",
    "            results.append((i, similarity, self.documents[i]))\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return results[:top_k]\n",
    "        else:\n",
    "            return results\n",
    "\n",
    "class ChineseBERTRetrieval(BERTRetrieval):\n",
    "    def __init__(self, model_path: str = \"bert-base-chinese\", **kwargs):\n",
    "        super().__init__(model_path=model_path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35960128-f087-4383-8034-9eb353b0d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3_emb\n",
    "class QwenEmbeddingRetrieval:\n",
    "    def __init__(self, model_path, task_description=None):\n",
    "        self.model_path = model_path\n",
    "        self.task_description = task_description or \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        self.model = None\n",
    "        self.documents = []\n",
    "        self.document_embeddings = None\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        self.model = LLM(model=self.model_path, task=\"embed\")\n",
    "\n",
    "    def get_detailed_instruct(self, query: str) -> str:\n",
    "        return f'Instruct: {self.task_description}\\nQuery:{query}'\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        self.documents = documents\n",
    "        input_texts = documents\n",
    "        outputs = self.model.embed(input_texts)\n",
    "        self.document_embeddings = torch.tensor([o.outputs.embedding for o in outputs])\n",
    "        return self.document_embeddings\n",
    "    \n",
    "    def score(self, query, doc_index):\n",
    "        if doc_index >= len(self.documents) or self.document_embeddings is None:\n",
    "            return 0\n",
    "        query_embedding = self._get_query_embedding(query)\n",
    "        doc_embedding = self.document_embeddings[doc_index]\n",
    "        similarity = torch.dot(query_embedding, doc_embedding).item()\n",
    "        return similarity\n",
    "    \n",
    "    def _get_query_embedding(self, query):\n",
    "        instructed_query = self.get_detailed_instruct(query)\n",
    "        outputs = self.model.embed([instructed_query])\n",
    "        query_embedding = torch.tensor([o.outputs.embedding for o in outputs])[0]\n",
    "        return query_embedding\n",
    "    \n",
    "    def search(self, query, top_k=None):\n",
    "        if self.document_embeddings is None:\n",
    "            raise ValueError(\"请先调用fit方法处理文档\")\n",
    "        query_embedding = self._get_query_embedding(query)\n",
    "        scores = (query_embedding @ self.document_embeddings.T).tolist()\n",
    "        results = []\n",
    "        for i, score in enumerate(scores):\n",
    "            results.append((i, score, self.documents[i]))\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return results[:top_k]\n",
    "        else:\n",
    "            return results\n",
    "    \n",
    "    def batch_search(self, queries, top_k=None):\n",
    "        if self.document_embeddings is None:\n",
    "            raise ValueError(\"请先调用fit方法处理文档\")\n",
    "        instructed_queries = [self.get_detailed_instruct(query) for query in queries]\n",
    "        outputs = self.model.embed(instructed_queries)\n",
    "        query_embeddings = torch.tensor([o.outputs.embedding for o in outputs])\n",
    "        batch_scores = (query_embeddings @ self.document_embeddings.T)\n",
    "        all_results = []\n",
    "        for i, scores in enumerate(batch_scores.tolist()):\n",
    "            results = []\n",
    "            for j, score in enumerate(scores):\n",
    "                results.append((j, score, self.documents[j]))\n",
    "            results.sort(key=lambda x: x[1], reverse=True)\n",
    "            if top_k is not None:\n",
    "                all_results.append(results[:top_k])\n",
    "            else:\n",
    "                all_results.append(results)\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ce41a-26f7-4966-bf48-8efd1435ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3_rerank\n",
    "class QwenReranker:\n",
    "    def __init__(self, model_path: str, task_description: str = None, max_length: int = 8192):\n",
    "        self.model_path = model_path\n",
    "        self.task_description = task_description or \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "        self.max_length = max_length\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.sampling_params = None\n",
    "        self.true_token = None\n",
    "        self.false_token = None\n",
    "        self.suffix_tokens = None\n",
    "        \n",
    "        # 初始化模型和tokenizer\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        number_of_gpu = torch.cuda.device_count()\n",
    "        self.model = LLM(\n",
    "            model=self.model_path,\n",
    "            tensor_parallel_size=number_of_gpu,\n",
    "            max_model_len=10000,\n",
    "            enable_prefix_caching=True,\n",
    "            gpu_memory_utilization=0.8\n",
    "        )\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        self.true_token = self.tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
    "        self.false_token = self.tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0, \n",
    "            max_tokens=1,\n",
    "            logprobs=20, \n",
    "            allowed_token_ids=[self.true_token, self.false_token],\n",
    "        )\n",
    "    \n",
    "    def format_instruction(self, instruction: str, query: str, doc: str) -> List[Dict]:\n",
    "        text = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"<Instruct>: {instruction}\\n\\n<Query>: {query}\\n\\n<Document>: {doc}\"\n",
    "            }\n",
    "        ]\n",
    "        return text\n",
    "    \n",
    "    def process_inputs(self, pairs: List[Tuple[str, str]]) -> List[TokensPrompt]:\n",
    "        messages = []\n",
    "        for query, doc in pairs:\n",
    "            message = self.format_instruction(self.task_description, query, doc)\n",
    "            messages.append(message)\n",
    "        tokenized_messages = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=False, enable_thinking=False\n",
    "        )\n",
    "        processed_messages = []\n",
    "        for ele in tokenized_messages:\n",
    "            truncated = ele[:self.max_length - len(self.suffix_tokens)]\n",
    "            processed = truncated + self.suffix_tokens\n",
    "            processed_messages.append(TokensPrompt(prompt_token_ids=processed))\n",
    "        return processed_messages\n",
    "    \n",
    "    def compute_scores(self, inputs: List[TokensPrompt]) -> List[float]:\n",
    "        outputs = self.model.generate(inputs, self.sampling_params, use_tqdm=False)\n",
    "        scores = []\n",
    "        for i in range(len(outputs)):\n",
    "            final_logits = outputs[i].outputs[0].logprobs[-1]\n",
    "            token_count = len(outputs[i].outputs[0].token_ids)\n",
    "            true_logit = final_logits.get(self.true_token, -10).logprob\n",
    "            false_logit = final_logits.get(self.false_token, -10).logprob\n",
    "            true_score = math.exp(true_logit)\n",
    "            false_score = math.exp(false_logit)\n",
    "            score = true_score / (true_score + false_score) if (true_score + false_score) > 0 else 0.0\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], top_k: Optional[int] = None) -> List[Tuple[int, float, str]]:\n",
    "        pairs = [(query, doc) for doc in documents]\n",
    "        inputs = self.process_inputs(pairs)\n",
    "        scores = self.compute_scores(inputs)\n",
    "        results = []\n",
    "        for i, (score, doc) in enumerate(zip(scores, documents)):\n",
    "            results.append((i, score, doc))\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        if top_k is not None:\n",
    "            return results[:top_k]\n",
    "        else:\n",
    "            return results\n",
    "    \n",
    "    def batch_rerank(self, queries: List[str], documents_list: List[List[str]], top_k: Optional[int] = None) -> List[List[Tuple[int, float, str]]]:\n",
    "        all_results = []\n",
    "        \n",
    "        for query, documents in zip(queries, documents_list):\n",
    "            results = self.rerank(query, documents, top_k)\n",
    "            all_results.append(results)\n",
    "        return all_results\n",
    "    \n",
    "    def score(self, query: str, document: str) -> float:\n",
    "        pairs = [(query, document)]\n",
    "        inputs = self.process_inputs(pairs)\n",
    "        scores = self.compute_scores(inputs)\n",
    "        return scores[0] if scores else 0.0\n",
    "    \n",
    "    def close(self):\n",
    "        if hasattr(self, 'model') and self.model is not None:\n",
    "            destroy_model_parallel()\n",
    "            self.model = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9230aeb-bc63-49eb-9972-30b4deb8a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    # 包含RAG技术概要的文档\n",
    "    {\n",
    "        \"content\": \"RAG（Retrieval-Augmented Generation）技术是一种结合检索和生成的混合模型。它的技术概要包括三个核心组件：检索器从大规模知识库中检索相关文档，生成器基于检索到的信息生成回答，重排模块对结果进行优化。RAG能够有效减少大语言模型的幻觉问题，提高回答的准确性和可信度。\",\n",
    "        \"contains_rag\": True,\n",
    "        \"description\": \"详细描述RAG技术概要\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"RAG模型的技术架构主要分为两个阶段：检索阶段使用Dense Passage Retrieval或BM25算法从外部知识源获取相关信息，生成阶段将检索结果与原始问题结合，通过seq2seq模型生成最终答案。这种检索增强的生成方式显著提升了模型在知识密集型任务上的表现。\",\n",
    "        \"contains_rag\": True,\n",
    "        \"description\": \"描述RAG技术架构\"\n",
    "    },\n",
    "\n",
    "    # 不包含RAG技术概要的文档\n",
    "    {\n",
    "        \"content\": \"深度学习是机器学习的一个分支，它基于人工神经网络，特别是深度神经网络。深度学习模型能够从大量数据中自动学习特征表示，在计算机视觉、自然语言处理等领域取得了突破性进展。\",\n",
    "        \"contains_rag\": False,\n",
    "        \"description\": \"关于深度学习的介绍\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Transformer架构是当前自然语言处理领域的主流模型，它基于自注意力机制，摒弃了传统的循环和卷积结构。BERT、GPT等预训练语言模型都是基于Transformer构建的，在各种NLP任务上表现出色。\",\n",
    "        \"contains_rag\": False,\n",
    "        \"description\": \"关于Transformer架构的介绍\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"知识图谱是一种结构化的语义知识库，用于描述现实世界中的实体及其关系。它通常采用三元组形式存储数据，在智能搜索、推荐系统、问答系统等应用中发挥着重要作用。\",\n",
    "        \"contains_rag\": False,\n",
    "        \"description\": \"关于知识图谱的介绍\"\n",
    "    }\n",
    "]\n",
    "\n",
    "query = \"RAG的技术概要\"\n",
    "\n",
    "doc_contents = [doc[\"content\"] for doc in documents]\n",
    "doc_labels = [f\"文档{i+1} ({'包含RAG' if doc['contains_rag'] else '不包含RAG'})\" for i, doc in enumerate(documents)]\n",
    "\n",
    "tfidf = TFIDF(use_jieba=True)\n",
    "tfidf.fit(doc_contents)\n",
    "results_tfidf = tfidf.search(query)\n",
    "\n",
    "bm25 = BM25(k1=1.5, b=0.75,use_jieba=True)\n",
    "bm25.fit(doc_contents)\n",
    "results_bm25 = bm25.search(query)\n",
    "\n",
    "w2v_retrieval = Word2VecRetrieval(use_jieba=True, vector_size=100, min_count=1)\n",
    "w2v_retrieval.fit(doc_contents)\n",
    "results_w2v = w2v_retrieval.search(query)\n",
    "\n",
    "model_bert = \"/root/lanyun-fs/models/Bert\"\n",
    "bert_retrieval = ChineseBERTRetrieval(model_bert)\n",
    "bert_retrieval.fit(doc_contents)\n",
    "results_bert = bert_retrieval.search(query)\n",
    "\n",
    "model_emb = \"/root/lanyun-fs/models/Qwen3-Embedding-0.6B\"\n",
    "qwen3_emb_retrieval = QwenEmbeddingRetrieval(model_emb)\n",
    "qwen3_emb_retrieval.fit(doc_contents)\n",
    "results_qwen3_emb = qwen3_emb_retrieval.search(query)\n",
    "\n",
    "model_rerank = \"/root/lanyun-fs/models/Qwen3-Reranker-0.6B\"\n",
    "with QwenReranker(model_rerank) as reranker:\n",
    "    results_rerank = reranker.rerank(query, doc_contents)\n",
    "\n",
    "print('results_tfidf', results_tfidf)\n",
    "print()\n",
    "print('results_bm25', results_bm25)\n",
    "print()\n",
    "print('results_w2v', results_w2v)\n",
    "print()\n",
    "print('results_bert', results_bert)\n",
    "print()\n",
    "print('results_qwen3_emb', results_qwen3_emb)\n",
    "print()\n",
    "print('results_rerank', results_rerank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
